# ğŸ§  Local LLM System

This project is a fully Dockerized **Retrieval-Augmented Generation (RAG)** system that lets you ask questions over documents in a folder â€” completely offline.

It features:
- ğŸ¤– **Ollama** for local LLM inference (e.g. Mistral, Phi-3)
- ğŸ” **LlamaIndex** for document indexing and retrieval
- ğŸ—‚ï¸ **ChromaDB** for fast vector storage
- âš¡ **FastAPI** backend with streaming support (SSE)
- ğŸŒ **Streamlit** frontend UI with live token streaming
- ğŸ”„ **Live reload & folder watching** for auto reindex

---

## ğŸ“¦ Features

- âœ… Document indexing from `./files` folder
- âœ… LLM query answering with sources
- âœ… Streamed response via Server-Sent Events (SSE)
- âœ… Hot-reload dev mode (`--reload` + volume mounts)
- âœ… Configurable LLM and embedding models via `.env`

---

## ğŸš€ Quickstart

### 1. Clone the repo

```bash
git clone https://github.com/reefwn/local-llm
cd local-llm
```

### 2. Pull required Ollama model

```bash
ollama pull mistral
```

### 3. Start all services
```bash
docker-compose up -d
```

### 4. Open your browser
- Streamlit UI: [http://localhost:8501](http://localhost:8501)
- FastAPI docs: [http://localhost:8000/docs](http://localhost:8000/docs)

## âš™ï¸ Environment Configuration

Create a .env file

```dotenv
MODEL_NAME=mistral
EMBEDDING_MODEL=intfloat/e5-small-v2
FILES_PATH=/app/files
OLLAMA_API=http://ollama:11434
CHROMA_HOST=chroma
CHROMA_PORT=8000
```

## ğŸ—‚ï¸ Folder Structure

```bash
.
â”œâ”€â”€ api/               # FastAPI backend + LlamaIndex logic
â”œâ”€â”€ ui/                # Streamlit frontend
â”œâ”€â”€ files/             # Drop your documents here (PDF, TXT, MD)
â”œâ”€â”€ Dockerfile.api     # API Dockerfile
â”œâ”€â”€ Dockerfile.ui      # UI Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.api.txt
â”œâ”€â”€ requirements.ui.txt
â””â”€â”€ .env
```

## ğŸ§ª Supported Models

Any [Ollama-supported](https://ollama.com/library) model like:
  - mistral, mistral:instruct, mistral:Q4_0
  - phi3:mini:Q4_0
  - gemma:2b, tinyllama:chat
  - llama2:7b, etc.

Make sure to ollama pull <model> before using it.

## â“ Example Query

> "Summarize all documents in simple terms."

Or:

> "What are the company leave policies mentioned in the PDFs?"

## â¤ï¸ Credits

Built with:
- [Ollama](https://ollama.com)
- [LlamaIndex](https://www.llamaindex.ai)
- [ChromaDB](https://www.trychroma.com)
- [Streamlit](https://streamlit.io)
- [FastAPI](https://fastapi.tiangolo.com)